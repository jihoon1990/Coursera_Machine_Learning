{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing logistic regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from logistic_classifier_func import logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "products = pd.read_csv('amazon_baby_subset.csv')\n",
    "# Fill N/A\n",
    "products = products.fillna({'review':''})  # fill in N/A's in the review column\n",
    "# Reomove Punctuation for Text Cleaning\n",
    "products['review'] = products['review'].astype('str')\n",
    "products['review_clean'] = products['review'].apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "# Add Sentiment Column\n",
    "products['sentiment'] = products['rating'].apply(lambda rating: 1 if rating >= 4 else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Stop Pacifier Sucking without tears with Thumb...\n",
       "1      Nature's Lullabies Second Year Sticker Calendar\n",
       "2      Nature's Lullabies Second Year Sticker Calendar\n",
       "3                          Lamaze Peekaboo, I Love You\n",
       "4    SoftPlay Peek-A-Boo Where's Elmo A Children's ...\n",
       "5                            Our Baby Girl Memory Book\n",
       "6    Hunnt&reg; Falling Flowers and Birds Kids Nurs...\n",
       "7    Blessed By Pope Benedict XVI Divine Mercy Full...\n",
       "8    Cloth Diaper Pins Stainless Steel Traditional ...\n",
       "9    Cloth Diaper Pins Stainless Steel Traditional ...\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['name'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly explore more of this dataset. The 'name' column indicates the name of the product. Here we list the first 10 products in the dataset. We then count the number of positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting Positive & Negative Reviews\n",
      "Number of Positive Reviews:  26579\n",
      "Number of Negative Reviews:  26493\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCounting Positive & Negative Reviews\")\n",
    "num_pos = sum(products['sentiment'] == 1)\n",
    "num_neg = sum(products['sentiment'] == -1)\n",
    "print(\"Number of Positive Reviews: \", num_pos)\n",
    "print(\"Number of Negative Reviews: \", num_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply text cleaning on the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important Words:                0\n",
      "0          baby\n",
      "1           one\n",
      "2         great\n",
      "3          love\n",
      "4           use\n",
      "5         would\n",
      "6          like\n",
      "7          easy\n",
      "8        little\n",
      "9          seat\n",
      "10          old\n",
      "11         well\n",
      "12          get\n",
      "13         also\n",
      "14       really\n",
      "15          son\n",
      "16         time\n",
      "17       bought\n",
      "18      product\n",
      "19         good\n",
      "20     daughter\n",
      "21         much\n",
      "22        loves\n",
      "23     stroller\n",
      "24          put\n",
      "25       months\n",
      "26          car\n",
      "27        still\n",
      "28         back\n",
      "29         used\n",
      "..          ...\n",
      "163     started\n",
      "164    anything\n",
      "165        last\n",
      "166     company\n",
      "167        come\n",
      "168    returned\n",
      "169       maybe\n",
      "170        took\n",
      "171       broke\n",
      "172       makes\n",
      "173        stay\n",
      "174     instead\n",
      "175        idea\n",
      "176        head\n",
      "177        said\n",
      "178        less\n",
      "179        went\n",
      "180     working\n",
      "181        high\n",
      "182        unit\n",
      "183       seems\n",
      "184     picture\n",
      "185  completely\n",
      "186        wish\n",
      "187      buying\n",
      "188      babies\n",
      "189         won\n",
      "190         tub\n",
      "191      almost\n",
      "192      either\n",
      "\n",
      "[193 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import Important words\n",
    "important_words = pd.read_json('important_words.json')\n",
    "print(\"Important Words: \", important_words)\n",
    "\n",
    "# Counting Important words in `review_clean` column\n",
    "for word in important_words[0].values.tolist():\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The products now contains one column for each of the 193 important_words. As an example, the column perfect contains a count of the number of times the word perfect occurs in each of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews contain 'perfect':  2955\n"
     ]
    }
   ],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda x: 1 if x >= 1 else 0)\n",
    "print(\"Number of reviews contain 'perfect': \", products['contains_perfect'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['intercept'] = 1\n",
    "feature_set = ['intercept'] + important_words[0].tolist()\n",
    "# Coverrt to ndarray\n",
    "feature_matrix = products[feature_set].as_matrix()\n",
    "sentiment = products['sentiment'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of feature matrix:  (53072, 194)\n"
     ]
    }
   ],
   "source": [
    "# Shape of feature matrix\n",
    "print(\"Size of feature matrix: \", feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating conditional probability with link function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link function is given by:\n",
    "\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ represents the word counts of important_words in the review  $\\mathbf{x}_i$. Complete the following function that implements the link function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    # YOUR CODE HERE\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    # YOUR CODE HERE\n",
    "    predictions = 1. / (1 + np.exp(-scores))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside. How the link function works with matrix algebra\n",
    "Since the word counts are stored as columns in feature_matrix, each $i$-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$: $$\n",
    "[\\text{feature_matrix}] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\\n",
    "h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^T h(\\mathbf{x}_i)$ is obtained by multiplying feature_matrix and the coefficient vector $\\mathbf{w}$. $$\n",
    "[\\text{score}] =\n",
    "[\\text{feature_matrix}]\\mathbf{w} =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\mathbf{w}\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T\\mathbf{w} \\\\\n",
    "h(\\mathbf{x}_2)^T\\mathbf{w} \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\\mathbf{w}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}^T h(\\mathbf{x}_1) \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compute derivative of log likelihood with respect to a single coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture: $$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "We will now write a function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n",
    "errors vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n",
    "feature vector containing $h_j(\\mathbf{x}_i)$ for all $i$.\n",
    "Complete the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main lecture, our focus was on the likelihood. In the advanced optional video, however, we introduced a transformation of this likelihood---called the log likelihood---that simplifies the derivation of the gradient and is more numerically stable. Due to its numerical stability, we will use the log likelihood instead of the likelihood to assess the algorithm.\n",
    "The log likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "We provide a function to compute the log likelihood for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores)))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking gradient steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement our own logistic regression. All we have to do is to write a gradient ascent function that takes gradient steps towards the optimum.\n",
    "Complete the following function to solve the logistic regression model using gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_1,w) using your predict_probability() function\n",
    "        # YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, initial_coefficients)\n",
    "\n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "\n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            # compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            # YOUR CODE HERE\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,j])\n",
    "\n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            # YOUR CODE HERE\n",
    "            coefficients[j] += step_size * derivative\n",
    "\n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us run the logistic regression solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -36780.91768478\n",
      "iteration   1: log likelihood of observed labels = -36775.13127954\n",
      "iteration   2: log likelihood of observed labels = -36769.34795095\n",
      "iteration   3: log likelihood of observed labels = -36763.56769899\n",
      "iteration   4: log likelihood of observed labels = -36757.79052366\n",
      "iteration   5: log likelihood of observed labels = -36752.01642492\n",
      "iteration   6: log likelihood of observed labels = -36746.24540276\n",
      "iteration   7: log likelihood of observed labels = -36740.47745714\n",
      "iteration   8: log likelihood of observed labels = -36734.71258803\n",
      "iteration   9: log likelihood of observed labels = -36728.95079539\n",
      "iteration  10: log likelihood of observed labels = -36723.19207918\n",
      "iteration  11: log likelihood of observed labels = -36717.43643934\n",
      "iteration  12: log likelihood of observed labels = -36711.68387583\n",
      "iteration  13: log likelihood of observed labels = -36705.93438858\n",
      "iteration  14: log likelihood of observed labels = -36700.18797754\n",
      "iteration  15: log likelihood of observed labels = -36694.44464262\n",
      "iteration  20: log likelihood of observed labels = -36665.77410742\n",
      "iteration  30: log likelihood of observed labels = -36608.66369535\n",
      "iteration  40: log likelihood of observed labels = -36551.86072283\n",
      "iteration  50: log likelihood of observed labels = -36495.36502431\n",
      "iteration  60: log likelihood of observed labels = -36439.17638957\n",
      "iteration  70: log likelihood of observed labels = -36383.29456460\n",
      "iteration  80: log likelihood of observed labels = -36327.71925271\n",
      "iteration  90: log likelihood of observed labels = -36272.45011567\n",
      "iteration 100: log likelihood of observed labels = -36217.48677511\n",
      "iteration 200: log likelihood of observed labels = -35684.56313478\n",
      "iteration 300: log likelihood of observed labels = -35181.56106069\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients=np.zeros(194),\n",
    "                                   step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture that class predictions for a data point $\\mathbf{x}$ can be computed from the coefficients $\\mathbf{w}$ using the following formula: $$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{x}_i^T\\mathbf{w} >; 0 \\\\\n",
    "      -1 & \\mathbf{x}_i^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "Now, we will write some code to compute class predictions. We will do this in two steps:\n",
    "* Step 1: First compute the scores using feature_matrix and coefficients using a dot product.\n",
    "* Step 2: Using the formula above, compute the class predictions from the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = np.dot(feature_matrix, coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the following code block for Step 2 to compute the class predictions using the scores obtained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Positive Sentiment:  21348\n"
     ]
    }
   ],
   "source": [
    "class_predictions = pd.DataFrame(scores)[0].apply(lambda x: 1 if x > 0 else -1)\n",
    "print(\"Predicted Positive Sentiment: \", (class_predictions.values > 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now measure the classification accuracy of the model. Recall from the lecture that the classification accuracy can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$\n",
    "\n",
    "Complete the following code block to compute the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "# Reviews   correctly classified = 39325\n",
      "# Reviews incorrectly classified = 13747\n",
      "# Reviews total                  = 53072\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.74\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy\n",
    "num_mistakes = (class_predictions != sentiment).sum() # YOUR CODE HERE\n",
    "num_correct = len(sentiment) - num_mistakes\n",
    "accuracy = num_correct/len(sentiment) # YOUR CODE HERE\n",
    "print(\"-----------------------------------------------------\")\n",
    "print('# Reviews   correctly classified =', num_correct)\n",
    "print('# Reviews incorrectly classified =', num_mistakes)\n",
    "print('# Reviews total                  =', len(products))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print('Accuracy = %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Which words contribute most to positive & negative sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Treat each coefficient as a tuple, i.e. (word, coefficient_value).\n",
    "* Sort all the (word, coefficient_value) tuples by coefficient_value in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficients = list(coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words[0].values, coefficients)]\n",
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Positive Words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the 10 words that have the most positive coefficient values. These words are associated with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.069275149999999647),\n",
       " ('love', 0.069004250000000072),\n",
       " ('easy', 0.06748420000000005),\n",
       " ('little', 0.046790450000000261),\n",
       " ('loves', 0.046414200000000044),\n",
       " ('well', 0.030355849999999917),\n",
       " ('perfect', 0.030355849999999917),\n",
       " ('old', 0.020272350000000126),\n",
       " ('nice', 0.018481399999999922),\n",
       " ('soft', 0.017954649999999985)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Negative Words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we repeat this exercise on the 10 most negative words. That is, we compute the 10 words that have the most negative coefficient values. These words are associated with negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('return', -0.027977950000000137),\n",
       " ('monitor', -0.028324099999999935),\n",
       " ('disappointed', -0.030054849999999821),\n",
       " ('back', -0.031589949999999895),\n",
       " ('even', -0.03347119999999984),\n",
       " ('get', -0.03396785000000007),\n",
       " ('work', -0.036195249999999971),\n",
       " ('money', -0.04141760000000029),\n",
       " ('product', -0.047452650000000124),\n",
       " ('would', -0.063811999999999619)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
