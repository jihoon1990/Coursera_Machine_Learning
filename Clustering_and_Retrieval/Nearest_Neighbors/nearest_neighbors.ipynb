{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from NN_func import load_sparse_csr\n",
    "from NN_func import word_count_by_name\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "wiki = pd.read_csv(\"people_wiki.csv\")\n",
    "wiki['length'] = wiki.apply(lambda x: len(x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "      <td>1479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "      <td>1466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  length  \n",
       "0  digby morrell born 10 october 1979 is a former...    1479  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...    1466  \n",
       "2  harpdog brown is a singer and harmonica player...    1297  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...    2364  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...    1167  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word count vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load in the word count vectors, define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count vector\n",
    "word_count = load_sparse_csr('people_wiki_word_count.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-to-index mapping is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_to_word = pd.read_json(\"people_wiki_map_index_to_word.json\", typ='series')\n",
    "idx_to_word = idx_to_word.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a functino to find word count easily by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count_by_name(data, count_vector, name):\n",
    "    wiki_id = data[data['name']==name].index[0]\n",
    "    return count_vector[wiki_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NearestNeighbors(metric='euclidean', algorithm = 'brute')\n",
    "model.fit(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row number of Obama's article\n",
    "wiki[wiki['name']=='Barack Obama']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 nearest neighbors of Barack Obama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row number of Obama's article\n",
    "distances, indices = model.kneighbors(word_count_by_name(wiki,word_count,'Barack Obama'), n_neighbors=10)\n",
    "nn_obama = wiki.loc[indices[0]]\n",
    "nn_obama['distance'] = distances[0]\n",
    "nn_obama[['name','distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barack Obama word count table\n",
    "obama_words = pd.DataFrame(word_count_by_name(wiki,word_count,'Barack Obama').toarray()[0],index=idx_to_word.index,columns=['count'])\n",
    "obama_words = obama_words.sort_values(by='count',ascending=False)\n",
    "print(\"Obama words: \\n\", obama_words.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the 10 people are politicians, but about half of them have rather tenuous connections with Obama, other than the fact that they are politicians.\n",
    "\n",
    "* Francisco Barrio is a Mexican politician, and a former governor of Chihuahua.\n",
    "* Walter Mondale and Don Bonker are Democrats who made their career in late 1970s.\n",
    "* Wynn Normington Hugh-Jones is a former British diplomat and Liberal Party official.\n",
    "* Andy Anstett is a former politician in Manitoba, Canada.\n",
    "\n",
    "Nearest neighbors with raw word counts got some things right, showing all politicians in the query result, but missed finer and important details.\n",
    "\n",
    "For instance, let's find out why Francisco Barrio was considered a close neighbor of Obama. To do this, let's look at the most frequently used words in each of Barack Obama and Francisco Barrio's pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row number of Francisco Barrio's article\n",
    "wiki[wiki['name']=='Francisco Barrio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = model.kneighbors(word_count_by_name(wiki,word_count,'Francisco Barrio'), n_neighbors=10)\n",
    "nn_barrio = wiki.loc[indices[0]]\n",
    "nn_barrio['distance'] = distances[0]\n",
    "nn_barrio[['name','distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrio_words = pd.DataFrame(word_count_by_name(wiki,word_count,'Francisco Barrio').toarray()[0],index=idx_to_word.index,columns=['count'])\n",
    "barrio_words = barrio_words.sort_values(by='count',ascending=False)\n",
    "print(\"Barrio words: \\n\", barrio_words.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's extract the list of most frequent words that appear in both Obama's and Barrio's documents. We've so far sorted all words from Obama and Barrio's articles by their word frequencies. We will now use a dataframe operation known as join. The join operation is very useful when it comes to playing around with data: it lets you combine the content of two tables using a shared column (in this case, the word column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Word\n",
    "combined_words = obama_words.join(barrio_words,rsuffix='.1')\n",
    "# Rename Columns\n",
    "combined_words = combined_words.rename(columns={'count':'Obama','count.1':'Barrio'})\n",
    "# Sort by obama\n",
    "combined_words = combined_words.sort_values(by='Obama', ascending = False)\n",
    "combined_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distance between Obama and George W. Bush: \\n\", euclidean_distances(word_count_by_name(wiki,word_count,'Barack Obama'),word_count_by_name(wiki,word_count,'George W. Bush'))[0])\n",
    "print(\"Distance between Obama and Joe Biden: \\n\", euclidean_distances(word_count_by_name(wiki,word_count,'Barack Obama'),word_count_by_name(wiki,word_count,'Joe Biden'))[0])\n",
    "print(\"Distance between George W. Bush and Joe Biden: \\n\", euclidean_distances(word_count_by_name(wiki,word_count,'Joe Biden'),word_count_by_name(wiki,word_count,'George W. Bush'))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the perceived commonalities between Obama and Barrio were due to occurrences of extremely frequent words, such as \"the\", \"and\", and \"his\". So nearest neighbors is recommending plausible results sometimes for the wrong reasons.\n",
    "\n",
    "To retrieve articles that are more relevant, we should focus more on rare words that don't happen in every article. **TF-IDF(term frequencyâ€“inverse document frequency)** is a feature representation that penalizes words that are too common. Let's use GraphLab Create's implementation of TF-IDF and repeat the search for the 10 nearest neighbors of Barack Obama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = load_sparse_csr('people_wiki_tf_idf.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN model\n",
    "model = NearestNeighbors(metric='euclidean', algorithm = 'brute')\n",
    "model.fit(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = model.kneighbors(word_count_by_name(wiki,tf_idf,'Barack Obama'), n_neighbors=100)\n",
    "nn_obama = wiki.loc[indices[0]]\n",
    "nn_obama['distance'] = distances[0]\n",
    "nn_obama[['name','distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the results are more plausible with the use of TF-IDF. Let's take a look at the word vector for Obama and Schilirio's pages. Notice that TF-IDF representation assigns a weight to each word. This weight captures relative importance of that word in the document. Let us sort the words in Obama's article by their TF-IDF weights; we do the same for Schiliro's article as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barack Obama word count table\n",
    "obama_words = pd.DataFrame(word_count_by_name(wiki,tf_idf,'Barack Obama').toarray()[0],index=idx_to_word.index,columns=['weight'])\n",
    "obama_words = obama_words.sort_values(by='weight',ascending=False)\n",
    "obama_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schiliro Obama word count table\n",
    "schiliro_words = pd.DataFrame(word_count_by_name(wiki,tf_idf,'Phil Schiliro').toarray()[0],index=idx_to_word.index,columns=['weight'])\n",
    "schiliro_words = schiliro_words.sort_values(by='weight',ascending=False)\n",
    "schiliro_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine these tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined words\n",
    "combined_words = obama_words.join(schiliro_words,rsuffix='.1')\n",
    "# Rename Columns\n",
    "combined_words = combined_words.rename(columns={'weight':'Obama','weight.1':'Barrio'})\n",
    "# Sort by obama\n",
    "combined_words = combined_words.sort_values(by='Obama', ascending = False)\n",
    "combined_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance\n",
    "# Obama and Bush\n",
    "print(\"Distance between Obama and George W. Bush: \\n\", euclidean_distances(word_count_by_name(wiki,tf_idf,'Barack Obama'),word_count_by_name(wiki,tf_idf,'George W. Bush'))[0])\n",
    "print(\"Distance between Obama and Joe Biden: \\n\", euclidean_distances(word_count_by_name(wiki,tf_idf,'Barack Obama'),word_count_by_name(wiki,tf_idf,'Joe Biden'))[0])\n",
    "print(\"Distance between George W. Bush and Joe Biden: \\n\", euclidean_distances(word_count_by_name(wiki,tf_idf,'Joe Biden'),word_count_by_name(wiki,tf_idf,'George W. Bush'))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of document\n",
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(wiki['length'], 50, color='k', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='Entire Wikipedia', zorder=3, alpha=0.8)\n",
    "plt.hist(nn_obama['length'], 50, color='r', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (Euclidean)', zorder=10, alpha=0.8)\n",
    "plt.axvline(x=wiki[wiki['name']=='Barack Obama']['length'].values[0], color='k', linestyle='--', linewidth=4,\n",
    "           label='Length of Barack Obama', zorder=2)\n",
    "plt.axvline(x=wiki[wiki['name']=='Joe Biden']['length'].values[0], color='g', linestyle='--', linewidth=4,\n",
    "           label='Length of Joe Biden', zorder=1)\n",
    "plt.axis([1000, 5500, 0, 0.004])\n",
    "\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.title('Distribution of document length')\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to the rest of Wikipedia, nearest neighbors of Obama are overwhemingly short, most of them being shorter than 2000 words. The bias towards short articles is not appropriate in this application as there is really no reason to favor short articles over long articles (they are all Wikipedia articles, after all). Many Wikipedia articles are 2500 words or more, and both Obama and Biden are over 2500 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Both word-count features and TF-IDF are proportional to word frequencies. While TF-IDF penalizes very common words, longer articles tend to have longer TF-IDF vectors simply because they have more words in them.\n",
    "\n",
    "To remove this bias, we turn to **cosine distances**: \n",
    "\n",
    "$$\n",
    "d(\\mathbf{x},\\mathbf{y}) = 1 - \\frac{\\mathbf{x}^T\\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}\n",
    "$$ \n",
    "\n",
    "Cosine distances let us compare word distributions of two articles of varying lengths.\n",
    "\n",
    "Let us train a new nearest neighbor model, this time with cosine distances. We then repeat the search for Obama's 100 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance model\n",
    "model_cosine= NearestNeighbors(metric='cosine', algorithm = 'brute')\n",
    "model_cosine.fit(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = model_cosine.kneighbors(word_count_by_name(wiki,tf_idf,'Barack Obama'), n_neighbors=100)\n",
    "nn_obama_cosine = wiki.loc[indices[0]]\n",
    "nn_obama_cosine['distance'] = distances[0]\n",
    "nn_obama_cosine[['name','distance','length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a glance at the above table, things look better. For example, we now see Joe Biden as Barack Obama's nearest neighbor! We also see Hillary Clinton on the list. This list looks even more plausible as nearest neighbors of Barack Obama.\n",
    "\n",
    "Let's make a plot to better visualize the effect of having used cosine distance in place of Euclidean on our TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cosine distance\n",
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(wiki['length'], 50, color='k', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='Entire Wikipedia', zorder=3, alpha=0.8)\n",
    "plt.hist(nn_obama['length'], 50, color='r', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (Euclidean)', zorder=10, alpha=0.8)\n",
    "plt.hist(nn_obama_cosine['length'], 50, color='b', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (cosine)', zorder=11, alpha=0.8)\n",
    "plt.axvline(x=wiki[wiki['name']=='Barack Obama']['length'].values[0], color='k', linestyle='--', linewidth=4,\n",
    "           label='Length of Barack Obama', zorder=2)\n",
    "plt.axvline(x=wiki[wiki['name']=='Joe Biden']['length'].values[0], color='g', linestyle='--', linewidth=4,\n",
    "           label='Length of Joe Biden', zorder=1)\n",
    "plt.axis([1000, 5500, 0, 0.004])\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.title('Distribution of document length')\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with cosine distances: tweets vs. long articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily ever after? Not so fast. Cosine distances ignore all document lengths, which may be great in certain situations but not in others. For instance, consider the following (admittedly contrived) example.\n",
    "\n",
    "```\n",
    "+--------------------------------------------------------+\n",
    "|                                             +--------+ |\n",
    "|  One that shall not be named                | Follow | |\n",
    "|  @username                                  +--------+ |\n",
    "|                                                        |\n",
    "|  Democratic governments control law in response to     |\n",
    "|  popular act.                                          |\n",
    "|                                                        |\n",
    "|  8:05 AM - 16 May 2016                                 |\n",
    "|                                                        |\n",
    "|  Reply   Retweet (1,332)   Like (300)                  |\n",
    "|                                                        |\n",
    "+--------------------------------------------------------+\n",
    "```\n",
    "How similar is this tweet to Barack Obama's Wikipedia article? Let's transform the tweet into TF-IDF features, using an encoder fit to the Wikipedia dataset. (That is, let's treat this tweet as an article in our Wikipedia dataset and see what happens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem with cosine distances: tweets vs. long articles\n",
    "tweet = {'act': 3.4597778278724887,\n",
    " 'control': 3.721765211295327,\n",
    " 'democratic': 3.1026721743330414,\n",
    " 'governments': 4.167571323949673,\n",
    " 'in': 0.0009654063501214492,\n",
    " 'law': 2.4538226269605703,\n",
    " 'popular': 2.764478952022998,\n",
    " 'response': 4.261461747058352,\n",
    " 'to': 0.04694493768179923}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_indices = [idx_to_word[idx_to_word.index==word].values[0] for word in tweet.keys()]\n",
    "tweet_tf_idf = csr_matrix((list(tweet.values()),([0]*len(word_indices), word_indices)),\n",
    "                          shape=(1, tf_idf.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances(word_count_by_name(wiki,tf_idf,'Barack Obama'),tweet_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cosine distances, the tweet is \"nearer\" to Barack Obama than everyone else, except for Joe Biden! This probably is not something we want. If someone is reading the Barack Obama Wikipedia page, would you want to recommend they read this tweet? Ignoring article lengths completely resulted in nonsensical results. In practice, it is common to enforce maximum or minimum document lengths. After all, when someone is reading a long article from The Atlantic, you wouldn't recommend him/her a tweet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
